{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyONiWUHFa7n6ehPN3CxDGpY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yTz7sEFtVRTx","executionInfo":{"status":"ok","timestamp":1676217369787,"user_tz":-330,"elapsed":52104,"user":{"displayName":"Shoaib Khan","userId":"14988763543957736148"}},"outputId":"97fd009f-faac-4a73-9348-0dbc676e6c69"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Connecting drive \n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# importing necessary modules\n","import numpy as np\n","import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0-X-dt0WS33","executionInfo":{"status":"ok","timestamp":1676217550101,"user_tz":-330,"elapsed":2107,"user":{"displayName":"Shoaib Khan","userId":"14988763543957736148"}},"outputId":"376be44d-c92f-4883-89d6-6d6a2f5b61ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Read in dataset\n","data = pd.read_csv(\"/content/drive/MyDrive/Fake News Data/Dataset/Final_Clean.csv\")"],"metadata":{"id":"teZWTwJIVhM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.head"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4od8h6hoWLav","executionInfo":{"status":"ok","timestamp":1676217570600,"user_tz":-330,"elapsed":432,"user":{"displayName":"Shoaib Khan","userId":"14988763543957736148"}},"outputId":"17b109f0-2a74-48a5-9f31-41c8bf141e31"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method NDFrame.head of        Unnamed: 0  Unnamed: 0.1              author  \\\n","0               0             0      Louis Jacobson   \n","1               1             1          D.L. Davis   \n","2               2             2         Yacob Reyes   \n","3               3             3  Samantha Putterman   \n","4               4             4       Maria Ramirez   \n","...           ...           ...                 ...   \n","17580       17580          5935  Samantha Putterman   \n","17581       17581          5936          Eric Litke   \n","17582       17582          5937      Ciara O'Rourke   \n","17583       17583          5938  Samantha Putterman   \n","17584       17584          5939      Ciara O'Rourke   \n","\n","                                               statement  \\\n","0      “We created more new jobs in two years than an...   \n","1      \"During my time in office, we've increased per...   \n","2      \"(Ron) DeSantis' bill would remove: background...   \n","3      “Female student-athletes in Florida need to pr...   \n","4      \"Under Biden, we have seen over 4.6 million en...   \n","...                                                  ...   \n","17580  Says popular food and drink items such as Kraf...   \n","17581  \"The vast majority of Wisconsin students canno...   \n","17582  AARP backed \"federal funding for Planned Paren...   \n","17583  Says drinking a \"6-ounce glass of tonic water ...   \n","17584  Says a photo shows \"Kurdish kids\" after Turkey...   \n","\n","                                                 article  \\\n","0      President Joe Biden has regularly touted how w...   \n","1      Does school aid make a difference in student p...   \n","2      As California’s Democratic governor, Gavin New...   \n","3      UPDATE 2/9: Public outcry over menstrual quest...   \n","4      In an effort to limit southwest border crossin...   \n","...                                                  ...   \n","17580                                                NaN   \n","17581                                                NaN   \n","17582                                                NaN   \n","17583                                                NaN   \n","17584                                                NaN   \n","\n","                                        source              date     target  \\\n","0                                    Joe Biden  February 7, 2023  half-true   \n","1                                   Tony Evers  February 7, 2023  half-true   \n","2                                 Gavin Newsom  February 6, 2023  half-true   \n","3                                       Tweets  February 3, 2023  half-true   \n","4                                     Chip Roy     • February 3,  half-true   \n","...                                        ...               ...        ...   \n","17580                           Facebook posts  October 23, 2019      false   \n","17581                        Jeremy Thiesfeldt  October 23, 2019      false   \n","17582  Association of Mature American Citizens  October 22, 2019      false   \n","17583                           Facebook posts  October 21, 2019      false   \n","17584                              Viral image  October 21, 2019      false   \n","\n","      BinaryTarget  BinaryNumTarget  \n","0             REAL                1  \n","1             REAL                1  \n","2             REAL                1  \n","3             REAL                1  \n","4             REAL                1  \n","...            ...              ...  \n","17580        FALSE                0  \n","17581        FALSE                0  \n","17582        FALSE                0  \n","17583        FALSE                0  \n","17584        FALSE                0  \n","\n","[17585 rows x 10 columns]>"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# checking for null values\n","data.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPQPTT9cXTnY","executionInfo":{"status":"ok","timestamp":1676217778043,"user_tz":-330,"elapsed":426,"user":{"displayName":"Shoaib Khan","userId":"14988763543957736148"}},"outputId":"36a365f5-b77f-4f39-9f0c-7810cb95c333"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Unnamed: 0            0\n","Unnamed: 0.1          0\n","author                0\n","statement            94\n","article            3071\n","source              210\n","date                  0\n","target              210\n","BinaryTarget          0\n","BinaryNumTarget       0\n","dtype: int64"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# combining all text to save in content column\n","data['content'] = data['author'] + ' ' + data['statement'] + ' ' + data['article']"],"metadata":{"id":"KpgiT1H0Xnhr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Lemmatization**"],"metadata":{"id":"9FS1VCB5WogA"}},{"cell_type":"code","source":["# loading nltk lemmatizer\n","w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n","lemmatizer = nltk.stem.WordNetLemmatizer()"],"metadata":{"id":"OGZGmT9nWk4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to lemmatize\n","def lemmatization(content):\n","    lemmatized_content = re.sub('[^a-zA-Z]',' ',content)  # removing symbols \n","    lemmatized_content = lemmatized_content.lower()       # converting to lowercase\n","    lemmatized_content = lemmatized_content.split()       # splitting words and removing stopwords\n","    lemmatized_content = [lemmatizer.lemmatize(word) for word in lemmatized_content if not word in stopwords.words('english')]\n","    lemmatized_content = ' '.join(lemmatized_content)     # joining words again\n","    return lemmatized_content"],"metadata":{"id":"ZeCGhgyUW6Z5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# downloading necessary modules\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A8tfHq5fYdNJ","executionInfo":{"status":"ok","timestamp":1676218099531,"user_tz":-330,"elapsed":20,"user":{"displayName":"Shoaib Khan","userId":"14988763543957736148"}},"outputId":"ef381005-8f3d-4981-9ccb-643270374c5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# applying lemmatization on content\n","data['content'] = data['content'].apply(lemmatization)"],"metadata":{"id":"p1CxQbU-YaXR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving the dataframe for future use\n","data.to_csv('/content/drive/MyDrive/Fake News Data/Dataset/Final_Processed.csv')"],"metadata":{"id":"fkeEWg4GZfPZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Vectorization using Bag of Words**"],"metadata":{"id":"CJQkDKi-aGCT"}},{"cell_type":"code","source":["# Obtain the total words present in the dataset\n","list_of_words = []\n","for i in data.content:\n","    for j in i:\n","        list_of_words.append(j)\n","len(list_of_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1QrceSqVZ4nM","executionInfo":{"status":"ok","timestamp":1676218950330,"user_tz":-330,"elapsed":904,"user":{"displayName":"Shoaib Khan","userId":"14988763543957736148"}},"outputId":"a1b2e279-48d5-4c8c-c5d1-0885f63395a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4847349"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# Obtain the total number of unique words (using set())\n","total_words = len(list(set(list_of_words)))\n","total_words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IS9Nd9Iua8ET","executionInfo":{"status":"ok","timestamp":1676218955502,"user_tz":-330,"elapsed":592,"user":{"displayName":"Shoaib Khan","userId":"14988763543957736148"}},"outputId":"bf11945b-d553-4b00-e184-56d852c8e2bc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["47905"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# length of maximum document will be needed to create word embeddings \n","nltk.download('punkt')\n","maxlen = -1\n","for doc in data.content:\n","    tokens = nltk.word_tokenize(doc)\n","    if(maxlen<len(tokens)):\n","        maxlen = len(tokens)\n","print(\"The maximum number of words in any document is =\", maxlen)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91Xu_E4dcEdw","executionInfo":{"status":"ok","timestamp":1676219083222,"user_tz":-330,"elapsed":23420,"user":{"displayName":"Shoaib Khan","userId":"14988763543957736148"}},"outputId":"a1de294e-9cdb-47d1-83de-58f6f3d6708b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["The maximum number of words in any document is = 1083\n"]}]},{"cell_type":"code","source":["# Create a tokenizer to tokenize the words and create sequences of tokenized words\n","from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n","\n","bagOfWords = Tokenizer(num_words = total_words)\n","bagOfWords.fit_on_texts(data['content'])\n","bagOfWords_sequences = bagOfWords.texts_to_sequences(data['content'])"],"metadata":{"id":"T0hkOvgDcKep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add padding can either be maxlen or smaller number maxlen = 40 seems to work well based on results\n","# Make sure all different samples have the same length (fillted 0 s for missing)\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","bagOfWords_padded = pad_sequences(bagOfWords_sequences,maxlen = 40, padding = 'post', truncating = 'post')"],"metadata":{"id":"qSuC7wo1dRfx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving model\n","import pickle\n","pickle.dump(bagOfWords, open('/content/drive/MyDrive/Fake News Data/Models/BagOfWords.pkl', 'wb'))"],"metadata":{"id":"2r85n4ZJdySf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Vectorization using Tfidf vectorizer**"],"metadata":{"id":"xWApuKxOieE4"}},{"cell_type":"code","source":["dataset = []\n","for i in range(0, len(data)):\n","    review = re.sub('[^a-zA-Z]', ' ', data['content'][i])\n","    review = review.lower()\n","    review = review.split()\n","    \n","    review = ' '.join(review)\n","    dataset.append(review)"],"metadata":{"id":"m51PVpUQicpN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TFidf Vectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_v = TfidfVectorizer(max_features=5000, ngram_range=(1,3))"],"metadata":{"id":"ktVofrRviQkw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = tfidf_v.fit_transform(dataset).toarray()\n","y = data['BinaryNumTarget']"],"metadata":{"id":"_BvqMuZGjE9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving model\n","import pickle\n","pickle.dump(tfidf_v, open('/content/drive/MyDrive/Fake News Data/Models/tfidf_v.pkl', 'wb'))"],"metadata":{"id":"fd8ChHmojHVz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Vectorization using Word2Vec**"],"metadata":{"id":"t_YY1bhojrL5"}},{"cell_type":"code","source":["import gensim"],"metadata":{"id":"I6iQembhjluy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = data[\"BinaryNumTarget\"].values\n","#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process\n","X = []\n","stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n","regexTokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n","for par in data[\"content\"].values:\n","    tmp = []\n","    sentences = nltk.sent_tokenize(par)\n","    for sent in sentences:\n","        sent = sent.lower()\n","        tokens = regexTokenizer.tokenize(sent)\n","        filtered_words = [w.strip() for w in tokens if len(w) > 1]\n","        tmp.extend(filtered_words)\n","    X.append(tmp)"],"metadata":{"id":"JhxDAtt-kU5F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Dimension of vectors we are generating\n","EMBEDDING_DIM = 200\n","\n","#Creating Word Vectors by Word2Vec Method (takes time...)\n","w2v_model = gensim.models.Word2Vec(sentences=X, size=EMBEDDING_DIM, window=10, min_count=1)"],"metadata":{"id":"Nun-CyXekINL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n","\n","# Tokenizing Text -> Repsesenting each word by a number\n","w2v_tokenizer = Tokenizer()\n","w2v_tokenizer.fit_on_texts(X)"],"metadata":{"id":"19HwmG9_kKQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving model\n","import pickle\n","pickle.dump(w2v_model, open('/content/drive/MyDrive/Fake News Data/Models/w2v_model.pkl', 'wb'))\n","pickle.dump(w2v_tokenizer, open('/content/drive/MyDrive/Fake News Data/Models/w2v_tokenizer.pkl', 'wb'))"],"metadata":{"id":"ySQOUpRDlgWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Sfp6BCoXl_HJ"},"execution_count":null,"outputs":[]}]}